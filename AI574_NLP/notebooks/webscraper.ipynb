{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Make sure you've put in lxml into your conda environment\n",
    "# BeautifulSoup4 uses lxml for its 'xml' parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_arxiv(query, max_results=5):\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    search_url = f\"search_query={query}&start=0&max_results={max_results}\"\n",
    "    url = base_url + search_url\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status() # Raise an exception\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"xml\")\n",
    "    entries = soup.find_all('entry')\n",
    "\n",
    "    papers = []\n",
    "    for entry in entries:\n",
    "        title = entry.title.text.strip()\n",
    "        summary = entry.summary.text.strip()\n",
    "        pdf_url = entry.find('link', title='pdf')['href']\n",
    "        papers.append({'title': title, 'summary': summary, 'pdf_url': pdf_url})\n",
    "\n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(pdf_url, save_dir=\"arxiv_papers\"):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    response = requests.get(pdf_url)\n",
    "    response.raise_for_status() # Raise an exception\n",
    "\n",
    "    paper_id = pdf_url.split('/')[-1]\n",
    "    file_path = os.path.join(save_dir, f\"{paper_id}.pdf\")\n",
    "\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    print(f\"Downloaded: {file_path}\")\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_papers_from_arxiv(query, max_results=5, save_dir=\"arxiv_papers\"):\n",
    "    papers = search_arxiv(query, max_results)\n",
    "\n",
    "    for paper in papers:\n",
    "        print(f\"Title: {paper['title']}\")\n",
    "        print(f\"Summary: {paper['summary']}\\n\")\n",
    "        print(f\"Paper URL: {paper['pdf_url']}\")\n",
    "        download_pdf(paper['pdf_url'], save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Reddit Entity Linking Dataset\n",
      "Summary: We introduce and make publicly available an entity linking dataset from\n",
      "Reddit that contains 17,316 linked entities, each annotated by three human\n",
      "annotators and then grouped into Gold, Silver, and Bronze to indicate\n",
      "inter-annotator agreement. We analyze the different errors and disagreements\n",
      "made by annotators and suggest three types of corrections to the raw data.\n",
      "Finally, we tested existing entity linking models that are trained and tuned on\n",
      "text from non-social media datasets. We find that, although these existing\n",
      "entity linking models perform very well on their original datasets, they\n",
      "perform poorly on this social media dataset. We also show that the majority of\n",
      "these errors can be attributed to poor performance on the mention detection\n",
      "subtask. These results indicate the need for better entity linking models that\n",
      "can be applied to the enormous amount of social media text.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2101.01228v2\n",
      "Downloaded: arxiv_papers\\2101.01228v2.pdf\n",
      "Title: Analysis of Moral Judgement on Reddit\n",
      "Summary: Moral outrage has become synonymous with social media in recent years.\n",
      "However, the preponderance of academic analysis on social media websites has\n",
      "focused on hate speech and misinformation. This paper focuses on analyzing\n",
      "moral judgements rendered on social media by capturing the moral judgements\n",
      "that are passed in the subreddit /r/AmITheAsshole on Reddit. Using the labels\n",
      "associated with each judgement we train a classifier that can take a comment\n",
      "and determine whether it judges the user who made the original post to have\n",
      "positive or negative moral valence. Then, we use this classifier to investigate\n",
      "an assortment of website traits surrounding moral judgements in ten other\n",
      "subreddits, including where negative moral users like to post and their posting\n",
      "patterns. Our findings also indicate that posts that are judged in a positive\n",
      "manner will score higher.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2101.07664v1\n",
      "Downloaded: arxiv_papers\\2101.07664v1.pdf\n",
      "Title: Entity Graphs for Exploring Online Discourse\n",
      "Summary: Vast amounts of human communication occurs online. These digital traces of\n",
      "natural human communication along with recent advances in natural language\n",
      "processing technology provide for computational analysis of these discussions.\n",
      "In the study of social networks the typical perspective is to view users as\n",
      "nodes and concepts as flowing through and among the user-nodes within the\n",
      "social network. In the present work we take the opposite perspective: we\n",
      "extract and organize massive amounts of group discussion into a concept space\n",
      "we call an entity graph where concepts and entities are static and human\n",
      "communicators move about the concept space via their conversations. Framed by\n",
      "this perspective we performed several experiments and comparative analysis on\n",
      "large volumes of online discourse from Reddit. In quantitative experiments, we\n",
      "found that discourse was difficult to predict, especially as the conversation\n",
      "carried on. We also developed an interactive tool to visually inspect\n",
      "conversation trails over the entity graph; although they were difficult to\n",
      "predict, we found that conversations, in general, tended to diverge to a vast\n",
      "swath of topics initially, but then tended to converge to simple and popular\n",
      "concepts as the conversation progressed. An application of the spreading\n",
      "activation function from the field of cognitive psychology also provided\n",
      "compelling visual narratives from the data.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2304.03351v1\n",
      "Downloaded: arxiv_papers\\2304.03351v1.pdf\n",
      "Title: TK-KNN: A Balanced Distance-Based Pseudo Labeling Approach for\n",
      "  Semi-Supervised Intent Classification\n",
      "Summary: The ability to detect intent in dialogue systems has become increasingly\n",
      "important in modern technology. These systems often generate a large amount of\n",
      "unlabeled data, and manually labeling this data requires substantial human\n",
      "effort. Semi-supervised methods attempt to remedy this cost by using a model\n",
      "trained on a few labeled examples and then by assigning pseudo-labels to\n",
      "further a subset of unlabeled examples that has a model prediction confidence\n",
      "higher than a certain threshold. However, one particularly perilous consequence\n",
      "of these methods is the risk of picking an imbalanced set of examples across\n",
      "classes, which could lead to poor labels. In the present work, we describe\n",
      "Top-K K-Nearest Neighbor (TK-KNN), which uses a more robust pseudo-labeling\n",
      "approach based on distance in the embedding space while maintaining a balanced\n",
      "set of pseudo-labeled examples across classes through a ranking-based approach.\n",
      "Experiments on several datasets show that TK-KNN outperforms existing models,\n",
      "particularly when labeled data is scarce on popular datasets such as CLINC150\n",
      "and Banking77. Code is available at https://github.com/ServiceNow/tk-knn\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2310.11607v1\n",
      "Downloaded: arxiv_papers\\2310.11607v1.pdf\n",
      "Title: Demonstration of On-Sky Calibration of Astronomical Spectra using a 25\n",
      "  GHz near-IR Laser Frequency Comb\n",
      "Summary: We describe and characterize a 25 GHz laser frequency comb based on a\n",
      "cavity-filtered erbium fiber mode-locked laser. The comb provides a uniform\n",
      "array of optical frequencies spanning 1450 nm to 1700 nm, and is stabilized by\n",
      "use of a global positioning system referenced atomic clock. This comb was\n",
      "deployed at the 9.2 m Hobby-Eberly telescope at the McDonald Observatory where\n",
      "it was used as a radial velocity calibration source for the fiber-fed\n",
      "Pathfinder near-infrared spectrograph. Stellar targets were observed in three\n",
      "echelle orders over four nights, and radial velocity precision of \\sim10 m/s\n",
      "(\\sim6 MHz) was achieved from the comb-calibrated spectra.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/1201.5125v2\n",
      "Downloaded: arxiv_papers\\1201.5125v2.pdf\n",
      "Title: A near infrared frequency comb for Y+J band astronomical spectroscopy\n",
      "Summary: Radial velocity (RV) surveys supported by high precision wavelength\n",
      "references (notably ThAr lamps and I2 cells) have successfully identified\n",
      "hundreds of exoplanets; however, as the search for exoplanets moves to cooler,\n",
      "lower mass stars, the optimum wave band for observation for these objects moves\n",
      "into the near infrared (NIR) and new wavelength standards are required. To\n",
      "address this need we are following up our successful deployment of an H\n",
      "band(1.45-1.7{\\mu}m) laser frequency comb based wavelength reference with a\n",
      "comb working in the Y and J bands (0.98-1.3{\\mu}m). This comb will be optimized\n",
      "for use with a 50,000 resolution NIR spectrograph such as the Penn State\n",
      "Habitable Zone Planet Finder. We present design and performance details of the\n",
      "current Y+J band comb.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/1209.3295v1\n",
      "Downloaded: arxiv_papers\\1209.3295v1.pdf\n",
      "Title: HetSeq: Distributed GPU Training on Heterogeneous Infrastructure\n",
      "Summary: Modern deep learning systems like PyTorch and Tensorflow are able to train\n",
      "enormous models with billions (or trillions) of parameters on a distributed\n",
      "infrastructure. These systems require that the internal nodes have the same\n",
      "memory capacity and compute performance. Unfortunately, most organizations,\n",
      "especially universities, have a piecemeal approach to purchasing computer\n",
      "systems resulting in a heterogeneous infrastructure, which cannot be used to\n",
      "compute large models. The present work describes HetSeq, a software package\n",
      "adapted from the popular PyTorch package that provides the capability to train\n",
      "large neural network models on heterogeneous infrastructure. Experiments with\n",
      "transformer translation and BERT language model shows that HetSeq scales over\n",
      "heterogeneous systems. HetSeq can be easily extended to other models like image\n",
      "classification. Package with supported document is publicly available at\n",
      "https://github.com/yifding/hetseq.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2009.14783v1\n",
      "Downloaded: arxiv_papers\\2009.14783v1.pdf\n",
      "Title: Posthoc Verification and the Fallibility of the Ground Truth\n",
      "Summary: Classifiers commonly make use of pre-annotated datasets, wherein a model is\n",
      "evaluated by pre-defined metrics on a held-out test set typically made of\n",
      "human-annotated labels. Metrics used in these evaluations are tied to the\n",
      "availability of well-defined ground truth labels, and these metrics typically\n",
      "do not allow for inexact matches. These noisy ground truth labels and strict\n",
      "evaluation metrics may compromise the validity and realism of evaluation\n",
      "results. In the present work, we discuss these concerns and conduct a\n",
      "systematic posthoc verification experiment on the entity linking (EL) task.\n",
      "Unlike traditional methodologies, which asks annotators to provide free-form\n",
      "annotations, we ask annotators to verify the correctness of annotations after\n",
      "the fact (i.e., posthoc). Compared to pre-annotation evaluation,\n",
      "state-of-the-art EL models performed extremely well according to the posthoc\n",
      "evaluation methodology. Posthoc validation also permits the validation of the\n",
      "ground truth dataset. Surprisingly, we find predictions from EL models had a\n",
      "similar or higher verification rate than the ground truth. We conclude with a\n",
      "discussion on these findings and recommendations for future evaluations.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2106.07353v1\n",
      "Downloaded: arxiv_papers\\2106.07353v1.pdf\n",
      "Title: Truth Social Dataset\n",
      "Summary: Formally announced to the public following former President Donald Trump's\n",
      "bans and suspensions from mainstream social networks in early 2022 after his\n",
      "role in the January 6 Capitol Riots, Truth Social was launched as an\n",
      "\"alternative\" social media platform that claims to be a refuge for free speech,\n",
      "offering a platform for those disaffected by the content moderation policies of\n",
      "the existing, mainstream social networks. The subsequent rise of Truth Social\n",
      "has been driven largely by hard-line supporters of the former president as well\n",
      "as those affected by the content moderation of other social networks. These\n",
      "distinct qualities combined with its status as the main mouthpiece of the\n",
      "former president positions Truth Social as a particularly influential social\n",
      "media platform and give rise to several research questions. However, outside of\n",
      "a handful of news reports, little is known about the new social media platform\n",
      "partially due to a lack of well-curated data. In the current work, we describe\n",
      "a dataset of over 823,000 posts to Truth Social and and social network with\n",
      "over 454,000 distinct users. In addition to the dataset itself, we also present\n",
      "some basic analysis of its content, certain temporal features, and its network.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2303.11240v1\n",
      "Downloaded: arxiv_papers\\2303.11240v1.pdf\n",
      "Title: Cross-Domain Neural Entity Linking\n",
      "Summary: Entity Linking is the task of matching a mention to an entity in a given\n",
      "knowledge base (KB). It contributes to annotating a massive amount of documents\n",
      "existing on the Web to harness new facts about their matched entities. However,\n",
      "existing Entity Linking systems focus on developing models that are typically\n",
      "domain-dependent and robust only to a particular knowledge base on which they\n",
      "have been trained. The performance is not as adequate when being evaluated on\n",
      "documents and knowledge bases from different domains.\n",
      "  Approaches based on pre-trained language models, such as Wu et al. (2020),\n",
      "attempt to solve the problem using a zero-shot setup, illustrating some\n",
      "potential when evaluated on a general-domain KB. Nevertheless, the performance\n",
      "is not equivalent when evaluated on a domain-specific KB. To allow for more\n",
      "accurate Entity Linking across different domains, we propose our framework:\n",
      "Cross-Domain Neural Entity Linking (CDNEL). Our objective is to have a single\n",
      "system that enables simultaneous linking to both the general-domain KB and the\n",
      "domain-specific KB. CDNEL works by learning a joint representation space for\n",
      "these knowledge bases from different domains. It is evaluated using the\n",
      "external Entity Linking dataset (Zeshel) constructed by Logeswaran et al.\n",
      "(2019) and the Reddit dataset collected by Botzer et al. (2021), to compare our\n",
      "proposed method with the state-of-the-art results. The proposed framework uses\n",
      "different types of datasets for fine-tuning, resulting in different model\n",
      "variants of CDNEL. When evaluated on four domains included in the Zeshel\n",
      "dataset, these variants achieve an average precision gain of 9%.\n",
      "\n",
      "Paper URL: http://arxiv.org/pdf/2210.15616v1\n",
      "Downloaded: arxiv_papers\\2210.15616v1.pdf\n"
     ]
    }
   ],
   "source": [
    "query = 'botzer'\n",
    "num = 10\n",
    "\n",
    "download_papers_from_arxiv(query=query, max_results=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai574_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
